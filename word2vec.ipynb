{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Descargar y uso de un modelo word2vec\n",
        "\n",
        "En la siguiente celda vamos a cargar un modelo preentrenado. Este modelo ha sido entrenado con el dataset [word2vec-google-news-300](https://huggingface.co/fse/word2vec-google-news-300). Este dataset está formado por noticias de Google con alrededor de 100 mil millones de palabras, y utilizando embeddings de 300 dimensiones.\n",
        "\n",
        "NOTA: el modelo es bastante pesado (más de un 1,6 Gb) y puede tardar varios minutos en cargar."
      ],
      "metadata": {
        "id": "4ekGj9WCUqmJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BWHv0kBeOlCN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11cabff8-b288-416c-a2b1-ae1cb525bae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "wv = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La variable *wv* es una especie de diccionario donde las claves son las palabras y los valores sus embeddings. Así, podríamos acceder al embedding de la palabra *perro* como se muestra a continuación. Podemos ver como, efectivamente, el embedding consta de 300 valores numéricos."
      ],
      "metadata": {
        "id": "UQK_K0ERVfER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solo mostramos los primeros 30 valores para no ocupar toda la pantalla\n",
        "wv['dog'][:30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bNheCHUV0Hy",
        "outputId": "d59c616e-6d4e-4118-f7a2-805874d738e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.05126953, -0.02233887, -0.17285156,  0.16113281, -0.08447266,\n",
              "        0.05737305,  0.05859375, -0.08251953, -0.01538086, -0.06347656,\n",
              "        0.1796875 , -0.42382812, -0.02258301, -0.16601562, -0.02514648,\n",
              "        0.10742188, -0.19921875,  0.15917969, -0.1875    , -0.12011719,\n",
              "        0.15527344, -0.09912109,  0.14257812, -0.1640625 , -0.08935547,\n",
              "        0.20019531, -0.14941406,  0.3203125 ,  0.328125  ,  0.02441406],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv['dog'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpV8HoUJZXNJ",
        "outputId": "ea429df3-08fc-4c6e-cba2-022ba90cc88c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Métricas de similitud: la distancia coseno\n",
        "\n",
        "Para medir la similitud entre vectores de alta dimensionalidad, como es el caso, se suele utilizar la distancia coseno, que se define como uno menos el coseno del ángulo que forman dichos vectores. Además, siempre va a ser una distancia entre 0 - máxima similitud - y 1 - mínima similitud.\n",
        "\n",
        "```\n",
        "cos_dist(u,v) = 1 - cos(u,v)\n",
        "```\n",
        "\n",
        "Así, si el ángulo entre *u* y *v* es 0º, la distancia coseno será\n",
        "\n",
        "```\n",
        "cos_dist(u,v) = 1 - cos(u,v) = 1 - cos(0º) = 1 - 1 = 0\n",
        "```\n",
        "\n",
        "Sin embargo, si el ángulo entre *u* y *w* es de 90º, la distancia coseno será\n",
        "\n",
        "```\n",
        "cos_dist(u,w) = 1 - cos(u,w) = 1 - cos(90º) = 1 - 0 = 1\n",
        "```\n",
        "\n",
        "Nota: cuando tenemos tantas dimensiones, los vectores tienden a ser casi ortogonales entre sí. Así, dos vectores sin relación semántica tenderán a tener una distancia cercana a 1.\n",
        "\n",
        "\n",
        "Así, la distancia coseno entre *perro* y *gato* será menor que entre *perro* y *guitarra*."
      ],
      "metadata": {
        "id": "LOVLA2C3V8qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos la función para calcular la distancia coseno\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Medimos la distancia entre \"perro\" y \"gato\"\n",
        "print(\"La distancia coseno entre 'perro' y 'gato' es de\", cosine(wv['dog'], wv['cat']).round(2))\n",
        "\n",
        "\n",
        "# Medimos la distancia entre \"perro\" y \"guitarra\"\n",
        "print(\"La distancia coseno entre 'perro' y 'gato' es de\", cosine(wv['dog'], wv['guitar']).round(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVqQhXpdSVkR",
        "outputId": "cbd23069-87ec-484a-bb67-88a17e018de0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La distancia coseno entre 'perro' y 'gato' es de 0.24\n",
            "La distancia coseno entre 'perro' y 'gato' es de 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como habíamos predicho, los embeddings correspondientes a *perro* y *gato* son much más parecidos que los correspondientes a *perro* y *guitarra*."
      ],
      "metadata": {
        "id": "AgMG0A0fZcxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relaciones semánticas\n",
        "\n",
        "Además, los vectores pueden representar relaciones semánticas. Por ejemplo, si tomamos el vector *rey*, extraemos el género masculino restándole el valor *hombre* y le añadimos el género femenino sumándole el vector *mujer*, obtenemos un vector muy cercano al que corresponde con la palabra *reina*."
      ],
      "metadata": {
        "id": "uPvwFFsKZqdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cosine(wv['king'] - wv['man'] + wv['woman'], wv['queen']).round(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eXzt-cGZMjy",
        "outputId": "5c8f5baa-2205-4f79-d3bc-b36ff2c5a657"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.27"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podríamos pensar que una distancia de 0.27 no es tan pequeña. El siguiente código va a calcular las similitudes entre la palabra *reina* y el resto de 3 millones de palabras - a excepción de *reina*, claro está - para obtener el embedding más cercano."
      ],
      "metadata": {
        "id": "TS7P3pjxhH0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_most_similar_word(target_word, target_embedding, verbose=False):\n",
        "\n",
        "    most_similar_word = None\n",
        "    min_dist = 99\n",
        "\n",
        "    counter = 1\n",
        "\n",
        "    vocab = wv.key_to_index.keys()\n",
        "\n",
        "    len_vocab = len(vocab)\n",
        "\n",
        "    for word in vocab:\n",
        "\n",
        "        if counter%500000==0 and verbose:\n",
        "            print(str(counter)+'/'+str(len_vocab))\n",
        "\n",
        "        if word != target_word:\n",
        "\n",
        "            dist = cosine(target_embedding, wv[word])\n",
        "\n",
        "            if dist < min_dist:\n",
        "                min_dist = dist\n",
        "                most_similar_word = word\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "    return min_dist, most_similar_word"
      ],
      "metadata": {
        "id": "TXPIwGcnUniY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_dist, most_similar_word = get_most_similar_word('queen', wv['queen'], verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2XJ-TDZiGWD",
        "outputId": "8cf8a6cc-b1ca-4e7c-8958-493de630f344"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500000/3000000\n",
            "1000000/3000000\n",
            "1500000/3000000\n",
            "2000000/3000000\n",
            "2500000/3000000\n",
            "3000000/3000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"La palabra más similar a 'queen' es {most_similar_word} con una distancia coseno de {min_dist.round(2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bxkrvh5dowh",
        "outputId": "543ec6b1-c831-4e0d-aee4-db710ef2266a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La palabra más similar a 'queen' es queens con una distancia coseno de 0.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos que la palabra con un embeding más cercano a *reina* es su plural, *reinas*, con una distancia apenas un 0.01 menor a nuestro resultado de las operaciones con los vectores *rey*, *hombre* y *mujer*, con lo que efectivamente, es una reconstrucción bastante buena del vector *reina*.\n",
        "\n",
        "Vamos ahora a coger el vector más similar a nuestra reconstrucción del vector *reina* - sin contar el vector original, *rey* - y observaremos que, efectivamente, es *reina*."
      ],
      "metadata": {
        "id": "eb0bPYkHhiNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_dist, most_similar_word = get_most_similar_word('king', wv['king'] - wv['man'] + wv['woman'], verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9XFHt_zgjDA",
        "outputId": "dbfef659-f37b-4937-ccb2-79cc173ef9f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500000/3000000\n",
            "1000000/3000000\n",
            "1500000/3000000\n",
            "2000000/3000000\n",
            "2500000/3000000\n",
            "3000000/3000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"La palabra más similar es {most_similar_word} con una distancia coseno de {min_dist.round(2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbuHf48dkcpC",
        "outputId": "6521d6ef-eb41-4372-ea6f-be2b6cb5443a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La palabra más similar es queen con una distancia coseno de 0.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sesgos en el lenguaje\n",
        "\n",
        "Explorando este tipo de relaciones semánticas, es interesante ver ciertos sesgos en el lenguaje, por ejemplo, relacionados con el género, como se exploran en [este paper](https://arxiv.org/abs/1607.06520). Por ejemplo, si tomamos el vector de *programador*, le restamos el de *hombre* y le sumamos el de *mujer*, obtenemos que el vector más cercano es de le *ama de casa*."
      ],
      "metadata": {
        "id": "OKupzNZqmfZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_dist, most_similar_word = get_most_similar_word('computer_programmer', wv['computer_programmer'] - wv['man'] + wv['woman'], verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xntMLqJukd4h",
        "outputId": "99681b4e-1e43-479a-dd30-52648b15907e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500000/3000000\n",
            "1000000/3000000\n",
            "1500000/3000000\n",
            "2000000/3000000\n",
            "2500000/3000000\n",
            "3000000/3000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"La palabra más similar es {most_similar_word} con una distancia coseno de {min_dist.round(2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSWmjrNXmvWL",
        "outputId": "a79d0cfa-89bf-45fe-f6cd-a3d16a32c713"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La palabra más similar es homemaker con una distancia coseno de 0.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En otro ejemplo, se tomaba el vector de *médico*, se restaba el de *abuelo* y se sumaba el de *abuela*, obteniendo *enfermera* como vector más cercano."
      ],
      "metadata": {
        "id": "KEQ9mWS2pGeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_dist, most_similar_word = get_most_similar_word('doctor', wv['doctor'] - wv['grandfather'] + wv['grandmother'], verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTl4X0FPnANR",
        "outputId": "a9f7adec-9c4b-4951-e6dd-ea48731d5e09"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500000/3000000\n",
            "1000000/3000000\n",
            "1500000/3000000\n",
            "2000000/3000000\n",
            "2500000/3000000\n",
            "3000000/3000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"La palabra más similar es {most_similar_word} con una distancia coseno de {min_dist.round(2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BNrPkRCoDZr",
        "outputId": "dbe11cd3-9cb8-438c-fbd6-24b1540eb534"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La palabra más similar es nurse con una distancia coseno de 0.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto dio lugar a técnicas de eliminación de sesgos. A grosso modo, lo que proponía el paper era tomar varios pares de palabras, de los cuales un elemento era masculino y otro femenino - *hombre* y *mujer*, *chico* y *chica*, etc -, calcular el vector de género promediando las diferencias y eliminar dicha componente de los embeddings."
      ],
      "metadata": {
        "id": "t8_ub5CWpIai"
      }
    }
  ]
}